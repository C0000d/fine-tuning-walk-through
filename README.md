# Flanâ€‘T5 Dialogueâ€‘Summary â™ full vsâ€¯LoRA fineâ€‘tuning comparison 
*Trainâ€‘time frugal flavourâ€‘tuning on **DialogSum** with ROUGE tracking*

---

##  What this repo contains

| Area | What I actually coded |
|------|-----------------------|
| **Dataset handling** | Loaded **`knkarthick/dialogsum`** via ğŸ¤—Â `datasets`; formulated a prompt wrapper and batchâ€‘tokenised every split with the base Flanâ€‘T5 tokenizer. |
| **Baseline check** | Ran zeroâ€‘shot generation from **`google/flanâ€‘t5â€‘base`** to record starting quality and inspect the prompt shape. |
| **Full fineâ€‘tune stub** | Oneâ€‘epoch / oneâ€‘step `Trainer`; shows the wiring for endâ€‘toâ€‘end FT, but keeps compute minimal. |
| **Parameterâ€‘Efficient FT** | Injected **LoRA** adapters (`r=32`, Q+V only) with `peft`; trained on the same data slice using a higher LR and `auto_find_batch_size`. |
| **Model bookkeeping** | Saved LoRA weights locally, plus a separate *fullyâ€‘fineâ€‘tuned* checkpoint previously trained on GPU (referenced as `./flan-dialogue-summary-checkpoint`). |
| **Evaluation harness** | â€¢ Generates summaries for 10Â test items with three systems (base / fullâ€‘FT / LoRA).<br>â€¢ Computes aggregated **ROUGEâ€‘1/2/L/Lsum** via `evaluate`.<br>â€¢ Helper to print absolute % deltas between systems. |

---

## ï¸  Quick start

```bash
# baseline & tokenised cache
python data_preparation.py

# run fine-tuning pipeline
python fine-tuning.py      # runs sections 1 and 2 âˆ’ baseline + fullâ€‘FT stub

# evaluation
python evaluation.py
```

---

##  Results

### ROUGE scores (higherâ€¯=â€¯better)

| Model            | ROUGEâ€‘1 | ROUGEâ€‘2 | ROUGEâ€‘L | ROUGEâ€‘Lsum |
|------------------|:-------:|:-------:|:-------:|:----------:|
| Original (Flanâ€‘T5) | 0.2334 | 0.0759 | 0.2014 | 0.2014 |
| Full FT (â€œInstructâ€) | **0.4212** | **0.1807** | **0.3384** | **0.3384** |
| LoRA PEFT         | 0.4080 | 0.1637 | 0.3251 | 0.3252 |

### Absolute improvement (percentage points)

| Comparison             | Î”â€¯ROUGEâ€‘1  | Î”â€¯ROUGEâ€‘2  | Î”â€¯ROUGEâ€‘L  | Î”â€¯ROUGEâ€‘Lsum |
|------------------------|:----------:|:----------:|:----------:|:------------:|
| InstructÂ vsÂ Original   | **+18.78** | **+10.48** | **+13.70** |  **+13.70**  |
| PEFTÂ vsÂ Original       |  +17.46â€¯   |   +8.78â€¯   |  +12.37â€¯   |   +12.37â€¯    |
| PEFTÂ vsÂ Instruct       |   âˆ’1.33â€¯   |   âˆ’1.70â€¯   |   âˆ’1.33â€¯   |    âˆ’1.32â€¯    |

### Trainable parameters & compute tradeâ€‘off

| Approach               | Trainableâ€¯Params | Totalâ€¯Params* | %â€¯Trainable | Key takeaway |
|------------------------|-----------------:|--------------:|------------:|--------------|
| Full fineâ€‘tune (â€œInstructâ€) | **247â€¯M** | 247â€¯M | **100â€¯%** | Highest accuracy, but youâ€™re updating every weight â†’ chunky GPU, long training time |
| LoRA PEFT              | 3.5â€¯M | 251â€¯M | **1.4â€¯%** | ~98â€¯% fewer weights touched. Tiny VRAM hit and superâ€‘fast to iterate, with only ~1â€“2â€¯pp ROUGEâ€‘L drop |

\* *Total params stay roughly the sameâ€”LoRA just adds a smidge of extra adapter weights on top of the frozen base.*

> **TL;DR:** If youâ€™ve got the hardware and need every last ROUGE point, full fineâ€‘tune wins. If youâ€™re skint on compute (or keen on rapid A/B testing), LoRA gets you 95â€¯% of the gains for 1â€¯% of the effort.
